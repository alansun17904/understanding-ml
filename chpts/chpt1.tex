\begin{problem}{1}
Show that given a training set $S=\{(xi,f(xi))\}_{i=1}^m \subset (\R^d \times \{0,1\})^m$, there exists a polynomial $p_S$ such that
$h_S(x)=1$ if and only if $p_S(x) \geq 0$, where $h_S$ is as defined in Equation (2.3).
\end{problem}
\begin{solution}
    Let $p_S(\bm{x}) = -\prod_{i: f(\bm{x_i}) = 1} (\bm{x} - \bm{x_i})^2$. Thus, $p_S$ is a polynomial and $h_S(\bm{x}) \geq 0$ if and only if there exists some $i \in [m]$ such that $\bm{x} = \bm{x_i}$ and $f(\bm{x}_i) = 1$. Therefore, learning the class of all thresholded polynomials using the ERM rule may lead to overfitting.
\end{solution}


\begin{problem}{2}
Let $\cH$ be a class of binary classifiers over a domain $\cX$. Let $\cD$ be an unknown distribution over $\cX$, and let $f$ be the target hypothesis in $\cH$. Fix some $h\in\cH$. Show that the expected value of $L_S(h)$ over the choice of $S|_x$ equals $L_{(\cD,f)}(h)$.
\end{problem}
\begin{solution}
    By definition, $L_{(\cD, f)}(h) = \P_{x \sim \cD}[h(x) \neq f(x)]$. And, $L_S(h) = \frac{|\{i : h(x_i) \neq f(x_i)\}|}{m}$. Applying the expectation over $S|_x$ to $L_S$, we have that 
    \begin{align*}
        \E L_S(h) &= \E\left[\frac{|\{i : h(x_i) \neq f(x_i)\}|}{m}\right] , \\
        &= \frac{1}{m} \E_{S|_x \sim \cD^m}\left[ \sum_{i=1}^m \ind_{h(x_i) \neq f(x_i)}\right], \\
        &= \frac{1}{m} \sum_{i=1}^m \E_{x_i \sim \cD} \ind_{h(x_i) \neq f(x_i)}, \tag{by independence} \\
        &= \frac{1}{m}\sum_{i=1}^m \P_{x \sim \cD}[h(x) \neq f(x)], \tag{by identically distributed} \\
        &= L_{(\cD, f)}(h).
    \end{align*}
\end{solution}

\begin{problem}{3}
    In this problem, we rely on the realizability assumption to analyze the hypothesis class of the set of all axis-aligned rectangles.
\end{problem}
\begin{solution} \
    \begin{enumerate}[label=(\alph*)]
        \item Since realizability is assumed, there must exist an axis-aligned rectangle that encloses all positive examples. Thus, any rectangle that only encloses the positive examples achieves zero training loss. Therefore, $A$ is an ERM algorithm.
        
        \item Let $R^\star = R(a_1^\star, a_2^\star, b_1^\star, b_2^\star)$ be the axis-aligned rectangle that generates all positive labeled examples. Any example $(x_1,x_2)$ that is positive labeled must satisfy $a_1^\star \leq x_1 \leq a_2^\star, b_1^\star \leq x_2 \leq b_2^\star$. Therefore, the smallest axis-aligned rectangle generated by algorithm $A$: $R_S = R(a_1, a_2, b_1, b_2)$ must also satisfy $a_1^\star \leq a_1, a_2 \leq a_2^\star, b_1^\star \leq b_1, b_2 \leq b_2^\star$. Thus, $R_S \subset R^\star$.

        Fix some distribution $\cD$ over $\cX$ and let $\nu_1, \nu_2, \nu_3, \nu_4$ be such that
        \[\P_{x \sim \cD}[x \in R^\star \cap R(\nu_1, a_2^\star, b_1^\star, b_2^\star)] = \ldots = \P_{x\sim\cD}[x \in R^\star \cap R(a_1^\star, a_2^\star, b_1^\star, \nu_4)] = \frac{\eps}{4}.\]
        For brevity, denote $R_1 = R(\nu_1, a_2^\star, b_1^\star, b_2^\star), R_2 = R(a_1^\star, \nu_2, b_1^\star, b_2^\star), R_3 = R(a_1^\star, a_2^\star, \nu_3, b_2^\star), R_4 = R(a_1^\star, a_2^\star, b_1^\star, \nu_4)$. If the training data $S$ contains positive examples in each $R_i$, then $A$ outputs an axis-aligned rectangle that must contain $R(\nu_1, \nu_2, \nu_3,\nu_4)$. And since $R_S \subset R^\star$, 
        \begin{align*}
            L_{(\cD, R^\star)}(R_S) &\leq L_{(\cD, R^\star)}(R(\nu_1,\nu_2,\nu_3,\nu_4)), \\
            &= \P_{x \sim \cD}[x \in R_1 \cup x \in R_2 \cup x \in R_3 \cup x \in R_4], \\
            &\leq \sum_{i=1}^4\P_{x \sim \cD}[x \in R_i], \tag{by union bound} \\
            &\leq \eps.
        \end{align*}
        Thus, as long as there exists positive-labeled training examples from $R_1,R_2,R_3,R_4$ in $S$, $A$ will incur at most $\eps$ error.
        
        By construction, for a given $R_i$, the probability that no training examples are sampled from this rectangle is $\left(1- \frac{\eps}{4}\right)^n$. Thus, by the union bound, the probability that no training examples are sampled from all the rectangles parameterized by $\nu_i$ is less than or equal to $4\left(1 - \frac{\eps}{4}\right)^n$. Then, 
        \begin{align*}
            \P_{S|_x \sim \cD^n}[L_{(\cD, R^\star)}(A(S)) > \eps] &= 4\left(1 - \frac{\eps}{4}\right)^n, \\
            &\leq 4 e^{-n\eps/4}. \tag{since $1-\eps \leq e^{-\eps}$}
        \end{align*}
        And so, for any $\delta > 0$, if $n \geq \frac{4\log(4/\delta)}{\eps}$, with probability $1-\delta$, $A$ will output a hypothesis that will incur an error of at most $\eps$. 

        \item Suppose now that the training points lie in $\R^d$ and that our hypothesis class $\cH$ are the set of axis-aligned rectangles in $\R^d$. Therefore, 
        \[
        \cH = \{R(x^1_1, x^1_2,\ldots,x^d_1,x^d_2):x^i_j \in \R, x^j_1 < x^j_2, \forall i \in [d], \forall j \in \{1,2\} \}.
        \]
        Let $A$ be the same algorithm that outputs the smallest rectangle that encloses all positive examples from the training data $S$. Let $R^\star = R({x^1_1}^\star, \ldots, {x^d_2}^\star)$. Thus, by our argument above $A(S) \subset R^\star$.
        
        Define $\nu^i_j$ where $i \in [d]$ and $j\in \{1,2\}$ such that $\P_{x \sim \cD}[x \in \R({{x^1_1}^\star,\ldots \nu^i_j,\ldots,{x^d_2}^\star})] \leq \frac{\eps}{2d}$. Label these rectangles $[2d]$. It follows that if $S$ contains positively-labeled examples from all $R_i$ and since $R_S \subset R^\star$, by the same derivation as above,
        \begin{align*}
            L_{(\cD, R^\star)}(R_S) &\leq L_{(\cD, R^\star)}(R(v_1^1,\ldots,v_2^d)), \\
            &= \P_{x\sim \cD}\left[x \in \bigcup_{i=1}^{2d} R_i\right], \\
            &\leq \sum_{i=1}^{2d} \cP_{x \sim \cD} \P[x \in \R_i], \tag{by union bound} \\
            &\leq \eps.
        \end{align*}
        Thus, as long as there exists positively-labeled training examples in all $R_i$ in $S$, $A$ will incur at most $\eps$ error. It remains to bound this probability. 

        By construction, the probability that no training examples are sampled a given $R_i$ is bounded above by $\left(1-\frac{\eps}{2d}\right)^n$. And by the union bound, the probability that no training examples are sampled from all $R_i$ is bounded above by $2d\left(1 - \frac{\eps}{2d}\right)^n$. Then, by the same computation above, for any $\eps > 0$, with probability $1 -\delta$, if $n > \frac{2d\log (2d/\delta)}{\eps}$, $A$ will output a hypothesis that will incur an error of at most $\eps$. 

        \item Consider the following implementation of $A$: 
        \begin{center}
        \fbox{%
        \begin{varwidth}{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}
        \begin{algorithmic}[1]
            \Procedure{SmallestAxisRect}{$\{(x_i, y_i)\}_{i=1}^n$}:  
            \For{$i \gets 1 \ldots 2d$}
                \If{$ i \mod 2 \equiv 0$}\State $\nu_i \gets \infty$
                \Else\State $\nu_i \gets -\infty$
                \EndIf
            \EndFor
            \For{$i\gets 1\ldots n$}
                \For{$j \gets 1 \ldots 2d$}
                    \If{$i \mod 2 \equiv 0$}\State{$\nu_i \gets \min(\nu_i, (x_i)_{j / 2})$}
                    \Else\State{$\nu_i \gets \max(\nu_i, (x_i)_{\ceil{j/2}})$}
                    \EndIf
                \EndFor
            \EndFor
            \EndProcedure
            \State \Return $R(\nu_1, \nu_2, \ldots, \nu_{2d})$
        \end{algorithmic}
        \end{varwidth}
        }
        \end{center}
        Thus, for each input point there are exactly $d$ iterations being performed. Therefore, given $n = \Theta\left(\frac{d\log(d/\delta)}{\eps}\right)$, the runtime of the algorithm is then $\Theta\left(\frac{d^2\log(d/\delta)}{\eps}\right)$.
    \end{enumerate}
\end{solution}